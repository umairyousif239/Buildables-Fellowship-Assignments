{
  "level1_count": 44,
  "level2_summary": "**Article Metadata:**\n*   **Title:** Deep Learning for Computer Vision: A Brief Review\n*   **Journal:** Hindawi Computational Intelligence and Neuroscience (Volume 2018, Article ID 7068349)\n*   **DOI:** 10.1155/2018/7068349\n*   **Authors:** Athanasios Voulodimos (corresponding), Nikolaos Doulamis, Anastasios Doulamis, Eftychios Protopapadakis.\n*   **Affiliations:** Technological Educational Institute of Athens, National Technical University of Athens.\n*   **Academic Editor:** Diego Andina.\n*   **Target Audience:** Computer vision, multimedia analysis, and general machine learning researchers.\n\n---\n\n**1. Introduction & Overview**\n    *   **1.1. Core Subject & Context:** Deep learning (DL) methods have significantly outperformed previous state-of-the-art machine learning techniques, particularly in computer vision.\n    *   **1.2. Deep Learning Definition & Rise:**\n        *   **Definition:** Computational models with multiple processing layers that learn and represent data with multiple levels of abstraction, mimicking brain perception. Includes neural networks, hierarchical probabilistic models, and feature learning algorithms.\n        *   **Rise:** Due to superior performance and abundance of complex data (visual, audio, medical).\n    *   **1.3. Historical Development & Growth Factors:**\n        *   **Historical Drivers:** Ambition to simulate the human brain (e.g., 1943 McCulloch-Pitts neuron model, LeNet, LSTM).\n        *   **Breakthrough (2006):** Hinton et al. with Deep Belief Networks (DBNs) using Restricted Boltzmann Machines (RBMs) and greedy unsupervised training.\n        *   **Factors Fueling Growth (Last Decade):**\n            *   **Data:** Large, high-quality, publicly available labeled datasets.\n            *   **Hardware:** Parallel GPU computing accelerating training.\n            *   **Algorithm Improvements:** Alleviation of vanishing gradient problem, new regularization (dropout, batch normalization, data augmentation).\n            *   **Software:** Powerful frameworks (TensorFlow, Theano, MXNet) for faster prototyping.\n    *   **1.4. Scope & Structure of Review:**\n        *   **Focus:** Key developments in DL architectures and algorithms for computer vision applications.\n        *   **Models Reviewed:** Convolutional Neural Networks (CNNs), 'Boltzmann family' (Deep Belief Networks, Deep Boltzmann Machines), Stacked (Denoising) Autoencoders.\n        *   **Models Excluded:** Long Short-Term Memory (LSTM) due to primary application in language/speech.\n        *   **Article Structure:** Section 2 reviews DL models; Section 3 describes DL contributions to computer vision tasks.\n\n**2. Deep Learning Models Reviewed**\n    *   **2.1. Convolutional Neural Networks (CNNs)**\n        *   **2.1.1. Architecture & Principles:**\n            *   **Inspiration:** Visual system's structure (precursor: Neocognitron). Key developer: Yann LeCun.\n            *   **Layers:**\n                *   **Convolutional Layers:** Utilize kernels to convolve input, generate feature maps. Detect elementary (edges, corners) to higher-order features. Employ \"Tied Weights\" (units in a plane share weights) to reduce parameters and increase generalization.\n                *   **Pooling Layers (Subsampling/Downsampling):** Reduce spatial dimensions, decrease computational overhead, prevent overfitting. Common types: average pooling, max pooling (faster convergence, better invariant features). Variations: stochastic pooling, spatial pyramid pooling, def-pooling.\n                *   **Fully Connected Layers:** Follow convolutional/pooling layers, perform high-level reasoning, convert 2D feature maps to 1D feature vectors for classification or further processing.\n            *   **Core Principles:** Local Receptive Fields, Tied Weights, Spatial Subsampling.\n        *   **2.1.2. Training & Mitigation:**\n            *   **Challenge:** Overfitting due to large number of parameters.\n            *   **Mitigation:** Stochastic pooling, dropout, data augmentation, pretraining (accelerates learning, enhances generalization).\n        *   **2.1.3. Performance & Applications:** Significantly outperform traditional machine learning in computer vision and pattern recognition due to exceptional performance and relative ease of training. Applications include face recognition, object detection, robotics vision, and self-driving cars.\n    *   **2.2. Deep Belief Networks (DBNs) & Deep Boltzmann Machines (DBMs)**\n        *   **2.2.1. Restricted Boltzmann Machine (RBM) Foundation:**\n            *   **Type:** Generative stochastic neural network, undirected graphical model.\n            *   **Structure:** Bipartite graph with stochastic visible (v) and hidden (h) variables (no intra-layer connections).\n            *   **Training:** Efficiently trained using gradient-based contrastive divergence algorithm.\n            *   **Mathematical:** Defined by an energy function E(v, h; θ) and joint distribution P(v, h; θ) with parameters θ (symmetric interaction terms W, bias terms a, b).\n        *   **2.2.2. Deep Belief Networks (DBNs):**\n            *   **Nature:** Probabilistic generative models, formed by stacking RBMs.\n            *   **Structure:** Top two layers form an undirected RBM, lower layers have directed, top-down connections.\n            *   **Training:**\n                1.  **Greedy Layer-wise Unsupervised Pre-training:** Each layer trained as an RBM, using output from previous layer as input.\n                2.  **Fine-tuning:** All weights jointly optimized using a proxy for DBN log-likelihood or a supervised criterion (often with an added classifier).\n            *   **Advantages:** Addresses parameter selection, prevents poor local optima, ensures appropriate network initialization.\n            *   **Disadvantages:** High computational training cost, unclear maximum likelihood optimization, inability to account for 2D image structure (limiting for computer vision).\n            *   **Variation:** Convolutional Deep Belief Network (CDBN) uses convolutional RBMs to incorporate spatial information, producing translation-invariant models.\n        *   **2.2.3. Deep Boltzmann Machines (DBMs):**\n            *   **Structure:** Multiple hidden layers with **all undirected connections**. Units in odd-numbered layers are conditionally independent of units in even-numbered layers.\n            *   **Inference:** Generally intractable.\n            *   **Training:** Initially joint training with Stochastic Maximum Likelihood (SML) (prone to poor local minima); improved by greedy layer-wise pretraining (stacking/training RBMs) followed by joint fine-tuning.\n            *   **Advantages:** Capture complex multi-layered representations, suitable for unsupervised learning, can be finetuned for supervised tasks. Unique approximate inference includes bottom-up and top-down feedback, effectively incorporating uncertainty. Allows joint optimization of all layer parameters for heterogeneous data.\n            *   **Drawback:** High computational cost of inference.\n    *   **2.3. Stacked (Denoising) Autoencoders (SDAEs)**\n        *   **2.3.1. Autoencoder (AE) Foundation:**\n            *   **Function:** Trained to encode input `x` into a representation `r(x)` such that `x` can be reconstructed from `r(x)`. Target output is the input itself.\n            *   **Representation `r(x)`:** Learned feature, aims to capture main data variations (similar to PCA but can be nonlinear).\n            *   **Objective:** Minimize reconstruction error (e.g., squared error, cross-entropy).\n            *   **Hidden Layers:** Linear layers behave like PCA; nonlinear layers capture multimodal aspects.\n        *   **2.3.2. Denoising Autoencoder (DAE):**\n            *   **Type:** Stochastic version of autoencoder.\n            *   **Mechanism:** Input is stochastically corrupted, but the uncorrupted input is used as the reconstruction target.\n            *   **Functions:** Encodes input, undoes corruption by capturing statistical dependencies.\n            *   **Theoretical Link:** Maximizes a lower bound on the log-likelihood of a generative model.\n            *   **Significance:** Demonstrated successful unsupervised pretraining and linked to generative models.\n        *   **2.3.3. Stacked (Denoising) Autoencoders (SDAEs):**\n            *   **Structure:** Formed by stacking denoising autoencoders, where the latent representation of a lower layer serves as input for the subsequent layer.\n            *   **Training:**\n                1.  **Unsupervised Pretraining:** One layer at a time, minimizing reconstruction error of its input.\n                2.  **Supervised Fine-tuning:** After pretraining, a logistic regression layer is added to the final output. The entire network (encoding parts only) is then trained like a multilayer perceptron, considering the target class.\n    *   **2.4. Model Comparisons & Characteristics:**\n        *   **SDAEs:** Flexible layer parametrization, not generative (cannot draw samples), can sometimes outperform DBNs, can work unsupervised, real-time training possible.\n        *   **DBNs:** Generative, often outperform SAs and other models for non-visual input, high computational cost, generally outperformed by CNNs on computer vision benchmarks.\n        *   **CNNs:** Automatic feature learning (no handcrafted features), invariant to transformations (translation, scale, rotation), generally outperform DBNs on computer vision tasks, but rely on labeled ground truth data, time-consuming training.\n        *   **Autoencoders (General):** Can be ineffective if initial errors lead to reconstructing the average of training data.\n        *   **Denoising Autoencoders:** Mitigate general autoencoder issues by learning to retrieve correct input from corrupted versions, grasping input distribution structure.\n\n**3. Computer Vision Applications**\n    *   **3.1. Object Detection:**\n        *   **Definition:** Detecting instances of semantic objects in digital images and video.\n        *   **Common Approach:** Candidate window generation, CNN feature extraction, classification (e.g., SVM).\n        *   **Foundational Paradigm:** Regions with CNN features (R-CNN) [32] (uses selective search for proposals).\n        *   **Improvements:** Joint object detection-semantic segmentation approaches [64-66] for better precision.\n        *   **DL Models:** Primarily CNNs [68, 9, 69], but also DBNs [70, 71], fused deep learning [72], semi-supervised models [73], and stacked autoencoders [74, 75] (e.g., for medical images, salient object detection).\n    *   **3.2. Face Recognition:**\n        *   **Historical:** Traditional systems relied on handcrafted features.\n        *   **Impact of CNNs:** Revolutionized the field due to feature learning and transformation invariance.\n        *   **Early Work:** [80] first to use CNNs for face recognition.\n        *   **State-of-the-Art (SOTA):** Light CNNs [81], VGG Face Descriptor [82].\n        *   **Other DL Models:** Convolutional DBN [44], Google's FaceNet [83] (CNN-based, triplet loss), Facebook's DeepFace [84] (CNN-based, 3D modeling, alignment), OpenFace [85] (open-source, CNN core, mobile-suitable).\n    *   **3.3. Action & Activity Recognition:**\n        *   **Significance:** A major research area, with many recent works utilizing deep learning.\n        *   **Applications:** Complex event detection/recognition [89], beach volleyball [90], large-scale video event classification [91], smartphone sensor data [92], fine-grained activity classification [13].\n        *   **Techniques:** CNNs are widely used; [12] enhances CNNs with a radius-margin bound for generalization.\n        *   **Emerging Strategy:** Fusing multimodal features and/or data (e.g., appearance + motion via multitask deep learning [93], heterogeneous features via AND/OR graphs [94], dual CNNs + LSTM for egocentric activity [95], combined CNN + LSTM [96], DBNs [97]).\n    *   **3.4. Human Pose Estimation:**\n        *   **Goal:** Determine the position of human joints from various inputs (images, sequences, depth, skeleton data).\n        *   **Challenges:** Wide range of human appearances, difficult illumination, cluttered backgrounds.\n        *   **Pre-DL:** Based on body part detection (e.g., pictorial structures [99]).\n        *   **Deep Learning Methods:**\n            *   **Holistic Methods:** Accomplish globally without explicit part models. Example: DeepPose [14] (joint regression problem), prone to inaccuracy in high-precision regions.\n            *   **Part-Based Methods:** Detect individual body parts, then use a graphical model for spatial information. Examples: CNNs learning conditional probabilities from local patches [15], multiple smaller CNNs for binary part classification [100], multiresolution CNNs for heat-map likelihood regression [101].\n\n**4. Datasets for Evaluation**\n    *   **Purpose:** Evaluate deep learning approaches, with content varying by application.\n    *   **Grayscale Images:** MNIST, NIST (handwritten digits).\n    *   **RGB Natural Images:** Caltech 101/256 (objects), CIFAR (color images), COIL (objects at 360-degree rotation).\n    *   **Hyperspectral Images:** SCIEN, AVIRIS sensor-based.\n    *   **Facial Characteristics Images:** Adience (age, gender), Labeled Faces in the Wild (LFW) (unconstrained face recognition).\n    *   **Medical Images:** Chest X-ray (112,120 images, 14 disease labels), Lymph Node Detection/Segmentation (CT images).\n    *   **Video Streams:** WR datasets (industrial tasks), YouTube-8M (8 million URLs with video-level labels).\n\n**5. Future Scope & Challenges**\n    *   **Computational Demands:** CNNs, DBNs, and DBMs are computationally demanding during training (Stacked Denoising Autoencoders can be trained in real time under certain circumstances).\n    *   **Significant Challenges:** Despite impressive results, a lack of theoretical groundwork remains.\n    *   **Key Issues for Research:** Defining optimal model type/structure for a given task, and profoundly comprehending the reasons for an architecture's or algorithm's effectiveness.\n\n**6. Funding & Acknowledgments**\n    *   **Funding:** IKY scholarships programme, co-financed by European Union (ESF) and Greek national funds under 'Reinforcement of Postdoctoral Researchers' (NSRF 2014-2020).\n    *   **Conflicts of Interest:** Authors declare no conflicts of interest.",
  "qa": [
    {
      "question": "(a) What are the main factors that contributed to the rise of deep learning in recent years? (b) Explain how GPU computing and large labeled datasets have influenced the performance of deep learning models. (c) What role did new activation functions and regularization techniques play in improving neural networks?",
      "answer": "Here are the answers based on the provided context:\n\n**(a) What are the main factors that contributed to the rise of deep learning in recent years?**\nThe main factors that contributed to the huge boost of deep learning are:\n1.  The appearance of large, high-quality, publicly available labeled datasets.\n2.  The empowerment of parallel GPU computing.\n3.  The alleviation of the vanishing gradient problem due to the disengagement from saturating activation functions (like hyperbolic tangent and the logistic function).\n\n**(b) Explain how GPU computing and large labeled datasets have influenced the performance of deep learning models.**\n*   **GPU computing:** It enabled the transition from CPU-based to GPU-based training, which significantly accelerated the training of deep models.\n*   **Large labeled datasets:** The availability of these datasets provided the necessary data for deep learning models to learn effectively and achieve high performance.\n\n**(c) What role did new activation functions and regularization techniques play in improving neural networks?**\n*   **New activation functions:** The disengagement from saturating activation functions (such as hyperbolic tangent and the logistic function) helped alleviate the vanishing gradient problem, which improved the training and performance of neural networks.\n*   **Regularization techniques:** The provided text does not mention the role of regularization techniques in improving neural networks.",
      "top_contexts": [
        {
          "text": "Table 1: Important milestones in the history of neural networks and machine learning, leading up to the era of deep learning.",
          "dist": 0.7332515716552734
        },
        {
          "text": "Among the most prominent factors that contributed to the huge boost of deep learning are the appearance of large, high-quality, publicly available labelled datasets, along with the empowermentofparallel GPU computing, which enabled the transition from CPU-based to GPU-based training thus allowing for significant acceleration in deep models' training. Additional factors may have played a lesser role as well, such as the alleviation of the vanishing gradient problem owing to the disengagement from saturating activation functions (such as hyperbolic tangent and the logistic function), the proposal",
          "dist": 0.780181884765625
        },
        {
          "text": "The ambition to create a system that simulates the human brain fueled the initial development of neural networks. In 1943, McCulloch and Pitts [1] tried to understand how the brain could produce highly complex patterns by using interconnected basic cells, called neurons. The McCulloch and Pitts model of a neuron, called a MCP model, has made an important contribution to the development of artificial neural networks. A series of major contributions in the field is presented in Table 1, including LeNet [2] and Long Short-Term Memory [3], leading up to today's 'era of deep learning.' One of the most substantial breakthroughs in deep learning came in 2006, when Hinton et al. [4] introduced the Deep Belief Network, with multiple layers of Restricted Boltzmann Machines, greedily training one layer at a time in an unsupervised way. Guiding the training of intermediate levels of representation using unsupervised learning, performed locally at each level, was the main principle behind a series of developments that brought about the last decade's surge in deep architectures and deep learning algorithms.",
          "dist": 0.8664007186889648
        },
        {
          "text": "2.4. Discussion. Some of the strengths and limitations of the presented deep learning models were already discussed in the respective subsections. In an attempt to compare these models (for a summary see Table 2), we can say that CNNs have generally performed better than DBNs in current literature on benchmark computer vision datasets such as MNIST. In cases where the input is nonvisual, DBNs often outperform other models, but the difficulty in accurately estimating joint probabilities as well as the computational cost in creating a DBNconstitutes drawbacks. A major positive aspect of CNNs is 'feature learning,' that is, the bypassing of handcrafted features, which are necessary for other types of networks; however, in CNNs features are automatically learned. On the other hand, CNNs rely on the availability of ground truth, that is, labelled training data, whereas DBNs/DBMs and SAs do not have this limitation and can work in an unsupervised manner. On a different note, one of the disadvantages of autoencoders lies in the fact that they could become ineffective if errors are present in the first layers. Such errors may cause the network to learn to reconstruct the average of the training data. Denoising autoencoders [56], however, can",
          "dist": 0.9553506970405579
        },
        {
          "text": "Overall, CNNs were shown to significantly outperform traditional machine learning approaches in a wide range of computer vision and pattern recognition tasks [33], examples of which will be presented in Section 3. Their exceptional performance combined with the relative easiness in training are the main reasons that explain the great surge in their popularity over the last few years.",
          "dist": 0.9709035158157349
        }
      ]
    },
    {
      "question": "(a) Describe the structure and functioning of Convolutional Neural Networks (CNNs). (b) Compare CNNs with Deep Belief Networks (DBNs) and Deep Boltzmann Machines (DBMs) in terms of learning approach and efficiency. (c) What are the main advantages and limitations of Stacked Autoencoders compared to the other two architectures?",
      "answer": "[Error answering: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit.\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 16.055587622s. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 250\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 16\n}\n]]",
      "top_contexts": [
        {
          "text": "As is easily seen, the principle for training stacked autoencoders is the same as the one previously described for Deep Belief Networks, but using autoencoders instead of Restricted Boltzmann Machines. A number of comparative experimental studies show that Deep Belief Networks tend to outperform stacked autoencoders ([58, 59]), but this is not always the case, especially when DBNs are compared to Stacked Denoising Autoencoders [56].",
          "dist": 0.5469427108764648
        },
        {
          "text": "2.4. Discussion. Some of the strengths and limitations of the presented deep learning models were already discussed in the respective subsections. In an attempt to compare these models (for a summary see Table 2), we can say that CNNs have generally performed better than DBNs in current literature on benchmark computer vision datasets such as MNIST. In cases where the input is nonvisual, DBNs often outperform other models, but the difficulty in accurately estimating joint probabilities as well as the computational cost in creating a DBNconstitutes drawbacks. A major positive aspect of CNNs is 'feature learning,' that is, the bypassing of handcrafted features, which are necessary for other types of networks; however, in CNNs features are automatically learned. On the other hand, CNNs rely on the availability of ground truth, that is, labelled training data, whereas DBNs/DBMs and SAs do not have this limitation and can work in an unsupervised manner. On a different note, one of the disadvantages of autoencoders lies in the fact that they could become ineffective if errors are present in the first layers. Such errors may cause the network to learn to reconstruct the average of the training data. Denoising autoencoders [56], however, can",
          "dist": 0.6727904677391052
        },
        {
          "text": "2.3. Stacked (Denoising) Autoencoders. Stacked Autoencoders use the autoencoder as their main building block, similarly to the way that Deep Belief Networks use Restricted Boltzmann Machines as component. It is therefore important to briefly present the basics of the autoencoder and its denoising version, before describing the deep learning architecture of Stacked (Denoising) Autoencoders.",
          "dist": 0.678098201751709
        },
        {
          "text": "2.2. Deep Belief Networks and Deep Boltzmann Machines. Deep Belief Networks and Deep Boltzmann Machines are deep learning models that belong in the 'Boltzmann family,' in the sense that they utilize the Restricted Boltzmann Machine (RBM) as learning module. The Restricted Boltzmann Machine (RBM) is a generative stochastic neural network. DBNs have undirected connections at the top two layers which form an RBM and directed connections to the lower layers. DBMs have undirected connections between all layers of the network. A graphic depiction of DBNs and DBMscanbefound in Figure 2. In the following subsections, we will describe the basic characteristics of DBNs and DBMs, after presenting their basic building block, the RBM.",
          "dist": 0.6991656422615051
        },
        {
          "text": "There are two main advantages in the above-described greedy learning process of theDBNs[40].First,ittackles the challenge of appropriate selection of parameters, which in some cases can lead to poor local optima, thereby ensuring that the network is appropriately initialized. Second, there is no requirement for labelled data since the process is unsupervised. Nevertheless, DBNs are also plagued by a number of shortcomings, such as the computational cost associated with training a DBN and the fact that the steps towards further optimization of the network based on maximum likelihood training approximation are unclear [41]. Furthermore, a significant disadvantage of DBNs is that they do not account for the two-dimensional structure of an input image, which may significantly affect their performance and applicability in computer vision and multimedia analysis problems. However, a later variation of the DBN, the Convolutional Deep Belief Network (CDBN) ([42, 43]), uses the spatial information of neighboring pixels by introducing convolutional RBMs, thus producing a translation invariant generative model that successfully scales when it comes to high dimensional images, as is evidenced in [44].",
          "dist": 0.8045853972434998
        }
      ]
    },
    {
      "question": "(a) Discuss how CNNs are applied in object detection and face recognition tasks. (b) What are some key differences between holistic and part-based methods in human pose estimation? (c) How have multimodal approaches improved performance in human activity recognition?",
      "answer": "Here's a breakdown of the answers based on the provided context:\n\n**(a) Discuss how CNNs are applied in object detection and face recognition tasks.**\nIn object detection, CNNs are commonly applied by first creating a large set of candidate windows. CNN features are then extracted from these windows and fed into a classifier (such as an SVM) to determine if the windows contain the target object. Many approaches are based on the concept of \"Regions with CNN features,\" where CNNs are used to process object proposals and classify them.\n\nThe provided text mentions \"face recognition\" as a key computer vision task where deep learning algorithms contribute, but it **does not provide details** on how CNNs are specifically applied in face recognition tasks.\n\n**(b) What are some key differences between holistic and part-based methods in human pose estimation?**\nThe key differences are:\n\n*   **Processing Approach:**\n    *   **Holistic methods** process input images in a global fashion, attempting to accomplish the task without explicitly defining models for individual body parts or their spatial relationships.\n    *   **Part-based methods** focus on detecting individual human body parts first, and then use a graphic model to incorporate spatial information among these parts.\n*   **Model Definition:**\n    *   **Holistic methods** (e.g., DeepPose) formulate human pose estimation as a joint regression problem and do not explicitly define graphical models or part detectors.\n    *   **Part-based methods** explicitly detect parts and then use models (like graphic models or weak spatial models) to understand their relationships and ensure consistency.\n*   **Accuracy:**\n    *   **Holistic methods** tend to be less accurate in high-precision regions due to the difficulty in directly learning complex pose vectors from images.\n    *   **Part-based methods** aim to improve accuracy by breaking down the problem into detecting individual parts and then assembling them with spatial constraints.\n\n**(c) How have multimodal approaches improved performance in human activity recognition?**\nThe provided text mentions \"action/activity recognition\" as a key computer vision task, but it **does not provide any information** on multimodal approaches or how they have improved performance in this area.",
      "top_contexts": [
        {
          "text": "Moving on to deep learning methods in human pose estimation, we can group them into holistic and part-based methods, depending on the way the input images are processed. The holistic processing methods tend to accomplish their task in a global fashion and do not explicitly define a model for each individual part and their spatial relationships. DeepPose [14] is a holistic model that formulates the human pose estimation method as a joint regression problem and does not explicitly define the graphical model or part detectors for the human pose estimation. Nevertheless, holisticbased methods tend to be plagued by inaccuracy in the high-precision region due to the difficulty in learning direct regression of complex pose vectors from images.",
          "dist": 0.7100364565849304
        },
        {
          "text": "3.1. Object Detection. Object detection is the process of detecting instances of semantic objects of a certain class (such as humans, airplanes, or birds) in digital images and video (Figure 4). A common approach for object detection frameworks includes the creation of a large set of candidate windows that are in the sequel classified using CNN features. For example, the method described in [32] employs selective search [60] to derive object proposals, extracts CNN features for each proposal, and then feeds the features to an SVM classifier to decide whether the windows include the object or not. A large number of works is based on the concept of Regions with CNN features proposed in [32]. Approaches following the Regions with CNN paradigm usually have good detection accuracies (e.g., [61, 62]); however, there is a significant number of methods trying to further improve the performance of Regions with CNN approaches, some of which succeed in finding approximate object positions but often cannot precisely determine the exact position of the object [63]. To this end, such methods often follow a joint object detection-semantic segmentation approach [64-66], usually attaining good results.",
          "dist": 0.7782557010650635
        },
        {
          "text": "A. Jain, J. Tompson, and M. Andriluka, 'Learning human pose estimation features with convolutional networks, ' in Proceedings of the ICLR , 2014.",
          "dist": 0.8630090355873108
        },
        {
          "text": "On the other hand, the part-based processing methods focus on detecting the human body parts individually, followed by a graphic model to incorporate the spatial information. In [15], the authors, instead of training the network using the whole image, use the local part patches and background patches to train a CNN, in order to learn conditional probabilities of the part presence and spatial relationships. In [100] the approach trains multiple smaller CNNs to perform independent binary body-part classification, followed with a higher-level weak spatial model to remove strong outliers and to enforce global pose consistency. Finally, in [101], a multiresolution CNN is designed to perform heat-map likelihood regression for each body part, followed with an implicit graphic model to further promote joint consistency.",
          "dist": 0.8751746416091919
        },
        {
          "text": "The remainder of this paper is organized as follows. In Section 2, the three aforementioned groups of deep learning model are reviewed: Convolutional Neural Networks, Deep Belief Networks and Deep Boltzmann Machines, and Stacked Autoencoders. The basic architectures, training processes, recent developments, advantages, and limitations of each group are presented. In Section 3, we describe the contribution of deep learning algorithms to key computer vision tasks, such as object detection and recognition, face recognition, action/activity recognition, and human pose estimation; we also provide a list of important datasets and resources for benchmarking and validation of deep learning algorithms. Finally, Section 4 concludes the paper with a summary of findings.",
          "dist": 0.8807322978973389
        }
      ]
    }
  ]
}